{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZjU4x1d5d0zl"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m/home/kesava89/repos/ssm-synthesis-data-processing/main.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://coder.de.soanpapdi.cloud/home/kesava89/repos/ssm-synthesis-data-processing/main.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://coder.de.soanpapdi.cloud/home/kesava89/repos/ssm-synthesis-data-processing/main.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch_geometric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Batch\n\u001b[1;32m      <a href='vscode-notebook-cell://coder.de.soanpapdi.cloud/home/kesava89/repos/ssm-synthesis-data-processing/main.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch_geometric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloader\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch_geometric.data import Batch\n",
        "from torch_geometric.loader import DataLoader\n",
        "import gpytorch\n",
        "\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from utils.classes import *\n",
        "from utils.exclude import *\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "filename = r'data/solid-state_dataset_20200713.json'\n",
        "filedata = open(filename, mode='r').read()\n",
        "jsonParse = json.loads(filedata)\n",
        "\n",
        "reactions = [from_dict(reaction, ReactionEntry) for reaction in jsonParse['reactions']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "periodic_table = {\n",
        "    'H': 1, 'He': 2, 'Li': 3, 'Be': 4, 'B': 5, 'C': 6, 'N': 7, 'O': 8, 'F': 9, 'Ne': 10,\n",
        "    'Na': 11, 'Mg': 12, 'Al': 13, 'Si': 14, 'P': 15, 'S': 16, 'Cl': 17, 'Ar': 18, 'K': 19, 'Ca': 20,\n",
        "    'Sc': 21, 'Ti': 22, 'V': 23, 'Cr': 24, 'Mn': 25, 'Fe': 26, 'Co': 27, 'Ni': 28, 'Cu': 29, 'Zn': 30,\n",
        "    'Ga': 31, 'Ge': 32, 'As': 33, 'Se': 34, 'Br': 35, 'Kr': 36, 'Rb': 37, 'Sr': 38, 'Y': 39, 'Zr': 40,\n",
        "    'Nb': 41, 'Mo': 42, 'Tc': 43, 'Ru': 44, 'Rh': 45, 'Pd': 46, 'Ag': 47, 'Cd': 48, 'In': 49, 'Sn': 50,\n",
        "    'Sb': 51, 'Te': 52, 'I': 53, 'Xe': 54, 'Cs': 55, 'Ba': 56, 'La': 57, 'Ce': 58, 'Pr': 59, 'Nd': 60,\n",
        "    'Pm': 61, 'Sm': 62, 'Eu': 63, 'Gd': 64, 'Tb': 65, 'Dy': 66, 'Ho': 67, 'Er': 68, 'Tm': 69, 'Yb': 70,\n",
        "    'Lu': 71, 'Hf': 72, 'Ta': 73, 'W': 74, 'Re': 75, 'Os': 76, 'Ir': 77, 'Pt': 78, 'Au': 79, 'Hg': 80,\n",
        "    'Tl': 81, 'Pb': 82, 'Bi': 83, 'Po': 84, 'At': 85, 'Rn': 86, 'Fr': 87, 'Ra': 88, 'Ac': 89, 'Th': 90,\n",
        "    'Pa': 91, 'U': 92, 'Np': 93, 'Pu': 94, 'Am': 95, 'Cm': 96, 'Bk': 97, 'Cf': 98, 'Es': 99, 'Fm': 100,\n",
        "    'Md': 101, 'No': 102, 'Lr': 103, 'Rf': 104, 'Db': 105, 'Sg': 106, 'Bh': 107, 'Hs': 108, 'Mt': 109,\n",
        "    'Ds': 110, 'Rg': 111, 'Cn': 112, 'Nh': 113, 'Fl': 114, 'Mc': 115, 'Lv': 116, 'Ts': 117, 'Og': 118,\n",
        "    'dummy':119\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def RemoveBadEntries(reactions: list,\n",
        "                        min_precursors = 2,\n",
        "                        remove_bad_doi = True,\n",
        "                        remove_bad_target = True,\n",
        "                        remove_bad_precursor = True, \n",
        "                        remove_duplicates_via_doi = True,\n",
        "                        remove_invalid_coefficients_multiplicities = True,\n",
        "                        use_bad_list = True,\n",
        "                        remove_negative_coefficients = True, \n",
        "                        verbose_output = True) -> List[ReactionEntry]:\n",
        "    \n",
        "    \"\"\"\n",
        "    Filters out bad reaction entries from a given list based on various criteria.\n",
        "    \n",
        "    Parameters:\n",
        "    reactions (list): List of ReactionEntry objects.\n",
        "    min_precursors (int): Minimum number of precursors required. Default is 2.\n",
        "    remove_bad_doi (bool): Flag to remove entries with bad DOIs. Default is True.\n",
        "    remove_bad_target (bool): Flag to remove entries with bad targets. Default is True.\n",
        "    remove_bad_precursor (bool): Flag to remove entries with bad precursors. Default is True.\n",
        "    remove_duplicates_via_doi (bool): Flag to remove duplicate entries via DOI. Default is True.\n",
        "    remove_invalid_coefficients_multiplicities (bool): Flag to remove entries with invalid coefficients or multiplicities. Default is True.\n",
        "    use_bad_list (bool): Flag to use a predefined list of bad entries. Default is True.\n",
        "    remove_negative_coefficients (bool): Flag to remove entries with negative coefficients. Default is True.\n",
        "    verbose_output (bool): Flag to enable verbose output. Default is True.\n",
        "    \n",
        "    Returns:\n",
        "    list: A list of filtered ReactionEntry objects.\n",
        "    \"\"\"\n",
        "        \n",
        "    filtered_reactions = []\n",
        "    bad_list = ['*', '-', 'x', '+', '/', 'ac', '(2N)', '(3N)', '(4N)', '(5N)', '(6N)', '7LiOH', '2Ni(OH)2']\n",
        "    isDigitRegex = re.compile(r'^-?\\d+(\\.\\d+)?$')\n",
        "    isNegativeRegex = re.compile(r'^(0*[1-9]\\d*|0*\\d*\\.\\d*[1-9])$')\n",
        "    RegexSelected = isNegativeRegex\n",
        "    if not remove_negative_coefficients:\n",
        "        RegexSelected = isDigitRegex\n",
        "    for reaction in reactions:\n",
        "        rxn: ReactionEntry = reaction\n",
        "        if (verbose_output): print(rxn.reaction_string, end='')\n",
        "        if remove_bad_doi and rxn.doi in BAD_DOI: \n",
        "            if (verbose_output): print(\": REJECTED DUE TO BAD DOI\")\n",
        "            continue\n",
        "        if len(rxn.precursors) < min_precursors: \n",
        "            if (verbose_output): print(\": REJECTED DUE TO LOW PRECURSOR COUNT\")\n",
        "            continue\n",
        "        if remove_bad_target and any(target in BAD_TARGETS for target in (rxn.targets_string)):  \n",
        "            if (verbose_output): print(\": REJECTED DUE TO BAD TARGET\")\n",
        "            continue\n",
        "        if remove_bad_precursor and  any(precursor.material_formula in BAD_PRECURSORS for precursor in rxn.precursors):  \n",
        "            if (verbose_output): print(\": REJECTED DUE TO BAD PRECURSOR\")\n",
        "            continue\n",
        "        # if any([not bool(isDigitRegex.match(s.amount)) for s in rxn.reaction.left_side]):\n",
        "        #     if (verbose_output): print(\": REJECTED DUE TO UNKNOWN COEFFICIENT IN LHS\")\n",
        "        #     continue\n",
        "        # if any([not bool(isDigitRegex.match(s.amount)) for s in rxn.reaction.right_side]):\n",
        "        #     if (verbose_output): print(\": REJECTED DUE TO UNKNOWN COEFFICIENT IN RHS\")\n",
        "        #     continue \n",
        "        if remove_invalid_coefficients_multiplicities and any([not bool(RegexSelected.match(s.amount)) for s in rxn.reaction.left_side]):\n",
        "            if (verbose_output): print(\": REJECTED DUE TO INVALID COEFFICIENT IN LHS\")\n",
        "            continue\n",
        "        if remove_invalid_coefficients_multiplicities and any([not bool(RegexSelected.match(s.amount)) for s in rxn.reaction.right_side]):\n",
        "            if (verbose_output): print(\": REJECTED DUE TO INVALID COEFFICIENT IN RHS\")\n",
        "            continue\n",
        "\n",
        "        found_bad = False\n",
        "        if use_bad_list: \n",
        "            for bad in bad_list:\n",
        "                if(any(bad in target_string for target_string in rxn.targets_string)) \\\n",
        "                or any(bad in precursor.material_formula for precursor in rxn.precursors):\n",
        "                    found_bad = True\n",
        "        \n",
        "        if found_bad:  \n",
        "            if (verbose_output): print(\": REJECTED CHARACTER FROM BAD LIST\")\n",
        "            continue\n",
        "        else:\n",
        "            if (verbose_output): print(\": SELECTED\") \n",
        "            filtered_reactions.append(rxn)\n",
        "    print(\"Filtered\", len(filtered_reactions), \"reactions out of total\", len(reactions))\n",
        "    return filtered_reactions\n",
        "\n",
        "def NormalizePrecursors(reactions: list) -> List[ReactionEntry]:\n",
        "    \n",
        "    \"\"\"\n",
        "    Normalizes precursor materials in the given list of reactions based on predefined replacements.\n",
        "    \n",
        "    Parameters:\n",
        "    reactions (list): List of ReactionEntry objects.\n",
        "    \n",
        "    Returns:\n",
        "    list: A list of ReactionEntry objects with normalized precursors.\n",
        "    \"\"\"\n",
        "\n",
        "    PrecursorMaterialReplacements = {}\n",
        "    for key, value in PREC_REPLACEMENTS.items():\n",
        "        PrecursorKey    = [ material for reaction in reactions for material in reaction.precursors if material.material_formula == key]\n",
        "        PrecursorValue  = [ material for reaction in reactions for material in reaction.precursors if material.material_formula == value]\n",
        "        filtered_reactions = [reaction for reaction in reactions for material in reaction.precursors if key in material.material_formula]\n",
        "        number_replacements = len(filtered_reactions)\n",
        "        if(len(PrecursorKey) > 0):\n",
        "            for rxn in filtered_reactions:\n",
        "                for prec in rxn.precursors:\n",
        "                    if prec.material_formula == PrecursorKey[0].material_formula: \n",
        "                        #print(\"replace here\")\n",
        "                        prec = PrecursorValue[0]\n",
        "                    if PrecursorKey[0].material_formula in rxn.reaction_string:\n",
        "                        rxn.reaction_string.replace(PrecursorKey[0].material_formula, PrecursorValue[0].material_formula)\n",
        "                    # TODO: You still have to replace Formula parts in rxn.reaction. Find a proposal that works.\n",
        "            PrecursorMaterialReplacements[PrecursorKey[0]] = PrecursorValue[0]\n",
        "            print(\"Processed:\", key, '=', value, \": replaced\", number_replacements, \" places\")\n",
        "        else: print(\"Skipped:\", key)\n",
        "    return reactions\n",
        "\n",
        "def RemoveDuplicates(reactions: list) -> List[ReactionEntry]:\n",
        "    \n",
        "    \"\"\"\n",
        "    Placeholder for function to remove duplicates from the list of reactions.\n",
        "    TODO: Integrate all other \"Remove*Duplicates\" Functions\n",
        "\n",
        "    Parameters:\n",
        "    reactions (list): List of ReactionEntry objects.\n",
        "    \n",
        "    Returns:\n",
        "    list: A list of ReactionEntry objects without duplicates.\n",
        "    \"\"\"\n",
        "\n",
        "    return reactions\n",
        "\n",
        "def RemoveDOIDuplicates(reactions: list) -> List[ReactionEntry]:\n",
        "    \n",
        "    \"\"\"\n",
        "    Removes duplicate reactions based on DOI and reaction string.\n",
        "    \n",
        "    Parameters:\n",
        "    reactions (list): List of ReactionEntry objects.\n",
        "    \n",
        "    Returns:\n",
        "    list: A list of ReactionEntry objects without DOI duplicates.\n",
        "    \"\"\"\n",
        "\n",
        "    # Assuming 'reactions' is your list of ReactionEntry objects\n",
        "\n",
        "    # Create a defaultdict to store entries grouped by (doi, reaction_string)\n",
        "    entry_dict = defaultdict(list)\n",
        "    for entry in reactions:\n",
        "        entry_dict[(entry.doi, entry.reaction_string)].append(entry)\n",
        "\n",
        "    # Filter out entries where there are duplicates (keep only the first occurrence)\n",
        "    filtered_reactions_doi = []\n",
        "    seen_keys = set()\n",
        "    for entry in reactions:\n",
        "        key = (entry.doi, entry.reaction_string)\n",
        "        if key not in seen_keys:\n",
        "            seen_keys.add(key)\n",
        "            filtered_reactions_doi.append(entry)\n",
        "    print(\"Filtered\", len(filtered_reactions_doi), \"reactions out of total\", len(reactions))\n",
        "    return filtered_reactions_doi\n",
        "\n",
        "def RemoveNodeMatchDuplicates(reactions: list) -> List[ReactionEntry]:\n",
        "    \n",
        "    \"\"\"\n",
        "    Removes duplicates based on node matches in the reaction entries.\n",
        "    \n",
        "    Parameters:\n",
        "    reactions (list): List of ReactionEntry objects.\n",
        "    \n",
        "    Returns:\n",
        "    list: A list of ReactionEntry objects without node match duplicates.\n",
        "    \"\"\"\n",
        "    # Dictionary to store entries grouped by (right_side_tuple, amount_tuple)\n",
        "    duplicate_entries = defaultdict(list)\n",
        "    filtered_reactions_dupForm = []\n",
        "    for entry in reactions:\n",
        "        # Create a tuple representation of right_side\n",
        "        right_side_tuple = tuple((part.amount, part.material) for part in entry.reaction.right_side)\n",
        "\n",
        "        # Create sets of (amount, material) tuples for target and precursors\n",
        "        target_materials = {(comp.amount, comp.formula) for comp in entry.target.composition}\n",
        "        precursor_materials =  {(mat.amount, mat.formula) for composition in (material.composition for material in reactions[0].precursors) for mat in composition}\n",
        "\n",
        "        # Create a tuple for (target materials, precursor materials)\n",
        "        materials_tuple = (frozenset(target_materials), frozenset(precursor_materials))\n",
        "\n",
        "        key = (right_side_tuple, materials_tuple)\n",
        "        duplicate_entries[key].append(entry)\n",
        "\n",
        "    # Now, find and print duplicates\n",
        "    seen_keys = set()\n",
        "    for key, entries in duplicate_entries.items():\n",
        "        if len(entries) > 1:\n",
        "            if key not in seen_keys:\n",
        "                seen_keys.add(key)\n",
        "                filtered_reactions_dupForm.append(entry)\n",
        "            print(len(entries), \"\\tDuplicates for: \", end='')\n",
        "            print(f\"Right Side: {key[0]}\", end='')\n",
        "            print(\"Target materials:\", end='')\n",
        "            print(key[1][0], end='')\n",
        "            print(\"Precursor materials:\", end='')\n",
        "            print(key[1][1], end='')\n",
        "            print()\n",
        "            for i in range(len(entries)):\n",
        "                entry = entries[i]\n",
        "                print(\"Entry #{}: \".format(i), end='')\n",
        "                calc_operations = [op for op in entry.operations if \"calc\" in op.token]\n",
        "                print(\"No. Calcination Operations: {}, \".format(len(calc_operations)))\n",
        "                for op in calc_operations:\n",
        "                    print(op)\n",
        "    print(\"Filtered\", len(filtered_reactions_dupForm), \"reactions out of total\", len(reactions))\n",
        "    return filtered_reactions_dupForm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered 20093 reactions out of total 31782\n",
            "Filtered 19318 reactions out of total 20093\n"
          ]
        }
      ],
      "source": [
        "badEntriesList = RemoveBadEntries(reactions, verbose_output = False)\n",
        "doiEntriesList = RemoveDOIDuplicates(badEntriesList)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_reaction(reaction_string):\n",
        "    \n",
        "    \"\"\"\n",
        "    Splits a chemical reaction string into reactants and products.\n",
        "\n",
        "    Parameters:\n",
        "    reaction_string (str): The reaction string to be split, with reactants and products separated by '=='.\n",
        "\n",
        "    Returns:\n",
        "    Tuple[str, str]: A tuple containing the reactants and products as separate strings.\n",
        "    \"\"\"\n",
        "        \n",
        "    reactants, products = reaction_string.split('==')\n",
        "    reactants = reactants.strip()\n",
        "    products = products.strip()\n",
        "    return reactants, products\n",
        "\n",
        "def parse_chemical_formula(formula):\n",
        "\n",
        "    \"\"\"\n",
        "    Parses a chemical formula into its constituent elements and their multiplicities.\n",
        "\n",
        "    Parameters:\n",
        "    formula (str): The chemical formula to be parsed.\n",
        "\n",
        "    Returns:\n",
        "    List[Tuple[str, str, str]]: A list of tuples, each containing an element, its multiplicity, and its atomic number.\n",
        "    \"\"\"\n",
        "\n",
        "    pattern = r'([A-Z][a-z]*)(\\d*\\.?\\d*)'\n",
        "    matches = re.findall(pattern, formula)\n",
        "    element_details = []\n",
        "    for match in matches:\n",
        "        element, multiplicity = match\n",
        "        multiplicity = multiplicity if multiplicity else '1'\n",
        "        atomic_number = re.findall(r'\\d+', multiplicity)\n",
        "        atomic_number = atomic_number[0] if atomic_number else '1'\n",
        "        element_details.append((element, multiplicity, atomic_number))\n",
        "    return element_details\n",
        "\n",
        "def extract_element_details(reaction):\n",
        "\n",
        "    \"\"\"\n",
        "    Extracts element details from a chemical reaction string.\n",
        "\n",
        "    Parameters:\n",
        "    reaction (str): The reaction string containing elements separated by '+'.\n",
        "\n",
        "    Returns:\n",
        "    List[Tuple[str, str, str]]: A list of tuples containing element details.\n",
        "    \"\"\"\n",
        "\n",
        "    parts = reaction.split('+')\n",
        "    element_details = []\n",
        "    for part in parts:\n",
        "        part = part.strip()\n",
        "        element_details.extend(parse_chemical_formula(part))\n",
        "    return element_details\n",
        "\n",
        "def expand_element_details(element_details, prefix):\n",
        "\n",
        "    \"\"\"\n",
        "    Expands element details into a dictionary with a given prefix.\n",
        "\n",
        "    Parameters:\n",
        "    element_details (List[Tuple[str, str, str]]): A list of tuples containing element details.\n",
        "    prefix (str): A prefix for the dictionary keys.\n",
        "\n",
        "    Returns:\n",
        "    Dict[str, str]: A dictionary with element details expanded into key-value pairs.\n",
        "    \"\"\"\n",
        "\n",
        "    data = {}\n",
        "    for i, detail in enumerate(element_details):\n",
        "        element, multiplicity, atomic_number = detail\n",
        "        data[f'{prefix}_element_{i+1}'] = element\n",
        "        data[f'{prefix}_multiplicity_{i+1}'] = multiplicity\n",
        "        data[f'{prefix}_atomic_number_{i+1}'] = atomic_number\n",
        "    return data\n",
        "\n",
        "def extract_temperatures(operations):\n",
        "\n",
        "    \"\"\"\n",
        "    Extracts sintering and calcination temperatures from a list of operations.\n",
        "\n",
        "    Parameters:\n",
        "    operations (List[Operation]): A list of operations, where each operation has a type and a token.\n",
        "\n",
        "    Returns:\n",
        "    Tuple[float, float]: A tuple containing sintering and calcination temperatures.\n",
        "    \"\"\"\n",
        "\n",
        "    sintering_temp = None\n",
        "    calcination_temp = None\n",
        "    for operation in operations:\n",
        "        if operation.type == 'HeatingOperation':\n",
        "            if operation.token == 'sintered':\n",
        "                sintering_temp = extract_temp(operation.conditions)\n",
        "            elif operation.token == 'calcined':\n",
        "                calcination_temp = extract_temp(operation.conditions)\n",
        "    return sintering_temp, calcination_temp\n",
        "\n",
        "def extract_temp(conditions):\n",
        "\n",
        "    \"\"\"\n",
        "    Extracts the temperature from a set of conditions.\n",
        "\n",
        "    Parameters:\n",
        "    conditions (Conditions): Conditions containing heating temperature data.\n",
        "\n",
        "    Returns:\n",
        "    float: The first heating temperature value found, or None if not found.\n",
        "    \"\"\"\n",
        "    \n",
        "    if conditions.heating_temperature:\n",
        "        for temp in conditions.heating_temperature:\n",
        "            if temp.values:\n",
        "                return temp.values[0]\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(19318, 336)\n"
          ]
        }
      ],
      "source": [
        "# Convert to DataFrame\n",
        "df = pd.DataFrame([{\n",
        "    'doi': entry.doi,\n",
        "    'paragraph_string': entry.paragraph_string,\n",
        "    'synthesis_type': entry.synthesis_type,\n",
        "    'reaction_string': entry.reaction_string,\n",
        "    'targets_string': entry.targets_string,\n",
        "    'sintering_temp': extract_temperatures(entry.operations)[0],\n",
        "    'calcination_temp': extract_temperatures(entry.operations)[1]\n",
        "} for entry in doiEntriesList])\n",
        "\n",
        "# Apply the function to create new columns\n",
        "df[['input_reaction', 'output_reaction']] = df['reaction_string'].apply(lambda x: pd.Series(split_reaction(x)))\n",
        "df['input_elements'] = df['input_reaction'].apply(extract_element_details)\n",
        "df['output_elements'] = df['output_reaction'].apply(extract_element_details)\n",
        "input_expanded = df['input_elements'].apply(lambda x: pd.Series(expand_element_details(x, 'input')))\n",
        "output_expanded = df['output_elements'].apply(lambda x: pd.Series(expand_element_details(x, 'output')))\n",
        "\n",
        "# Concatenate the expanded details with the original DataFrame\n",
        "df = pd.concat([df, input_expanded, output_expanded], axis=1)\n",
        "\n",
        "# Drop temporary columns\n",
        "df.drop(columns=['input_elements', 'output_elements'], inplace=True)\n",
        "\n",
        "print(np.array(df).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'openpyxl'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m/home/kesava89/repos/ssm-synthesis-data-processing/main.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://coder.de.soanpapdi.cloud/home/kesava89/repos/ssm-synthesis-data-processing/main.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m excel_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./reaction_entries.xlsx\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://coder.de.soanpapdi.cloud/home/kesava89/repos/ssm-synthesis-data-processing/main.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m df\u001b[39m.\u001b[39;49mto_excel(excel_path, index\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
            "File \u001b[0;32m~/repos/ssm-synthesis-data-processing/.conda/lib/python3.11/site-packages/pandas/core/generic.py:2345\u001b[0m, in \u001b[0;36mNDFrame.to_excel\u001b[0;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, inf_rep, freeze_panes, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   2332\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mformats\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexcel\u001b[39;00m \u001b[39mimport\u001b[39;00m ExcelFormatter\n\u001b[1;32m   2334\u001b[0m formatter \u001b[39m=\u001b[39m ExcelFormatter(\n\u001b[1;32m   2335\u001b[0m     df,\n\u001b[1;32m   2336\u001b[0m     na_rep\u001b[39m=\u001b[39mna_rep,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2343\u001b[0m     inf_rep\u001b[39m=\u001b[39minf_rep,\n\u001b[1;32m   2344\u001b[0m )\n\u001b[0;32m-> 2345\u001b[0m formatter\u001b[39m.\u001b[39;49mwrite(\n\u001b[1;32m   2346\u001b[0m     excel_writer,\n\u001b[1;32m   2347\u001b[0m     sheet_name\u001b[39m=\u001b[39;49msheet_name,\n\u001b[1;32m   2348\u001b[0m     startrow\u001b[39m=\u001b[39;49mstartrow,\n\u001b[1;32m   2349\u001b[0m     startcol\u001b[39m=\u001b[39;49mstartcol,\n\u001b[1;32m   2350\u001b[0m     freeze_panes\u001b[39m=\u001b[39;49mfreeze_panes,\n\u001b[1;32m   2351\u001b[0m     engine\u001b[39m=\u001b[39;49mengine,\n\u001b[1;32m   2352\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   2353\u001b[0m     engine_kwargs\u001b[39m=\u001b[39;49mengine_kwargs,\n\u001b[1;32m   2354\u001b[0m )\n",
            "File \u001b[0;32m~/repos/ssm-synthesis-data-processing/.conda/lib/python3.11/site-packages/pandas/io/formats/excel.py:946\u001b[0m, in \u001b[0;36mExcelFormatter.write\u001b[0;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m    942\u001b[0m     need_save \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    944\u001b[0m     \u001b[39m# error: Cannot instantiate abstract class 'ExcelWriter' with abstract\u001b[39;00m\n\u001b[1;32m    945\u001b[0m     \u001b[39m# attributes 'engine', 'save', 'supported_extensions' and 'write_cells'\u001b[39;00m\n\u001b[0;32m--> 946\u001b[0m     writer \u001b[39m=\u001b[39m ExcelWriter(  \u001b[39m# type: ignore[abstract]\u001b[39;49;00m\n\u001b[1;32m    947\u001b[0m         writer,\n\u001b[1;32m    948\u001b[0m         engine\u001b[39m=\u001b[39;49mengine,\n\u001b[1;32m    949\u001b[0m         storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    950\u001b[0m         engine_kwargs\u001b[39m=\u001b[39;49mengine_kwargs,\n\u001b[1;32m    951\u001b[0m     )\n\u001b[1;32m    952\u001b[0m     need_save \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    954\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[0;32m~/repos/ssm-synthesis-data-processing/.conda/lib/python3.11/site-packages/pandas/io/excel/_openpyxl.py:57\u001b[0m, in \u001b[0;36mOpenpyxlWriter.__init__\u001b[0;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     45\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     46\u001b[0m     path: FilePath \u001b[39m|\u001b[39m WriteExcelBuffer \u001b[39m|\u001b[39m ExcelWriter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     \u001b[39m# Use the openpyxl module as the Excel writer.\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mopenpyxl\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mworkbook\u001b[39;00m \u001b[39mimport\u001b[39;00m Workbook\n\u001b[1;32m     59\u001b[0m     engine_kwargs \u001b[39m=\u001b[39m combine_kwargs(engine_kwargs, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[1;32m     62\u001b[0m         path,\n\u001b[1;32m     63\u001b[0m         mode\u001b[39m=\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m         engine_kwargs\u001b[39m=\u001b[39mengine_kwargs,\n\u001b[1;32m     67\u001b[0m     )\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openpyxl'"
          ]
        }
      ],
      "source": [
        "excel_path = './reaction_entries.xlsx'\n",
        "df.to_excel(excel_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['doi']\n",
            "['paragraph', 'string']\n",
            "['synthesis', 'type']\n",
            "['reaction', 'string']\n",
            "['targets', 'string']\n",
            "['sintering', 'temp']\n",
            "['calcination', 'temp']\n",
            "['input', 'reaction']\n",
            "['output', 'reaction']\n",
            "['input', 'element', '1']\n",
            "['input', 'multiplicity', '1']\n",
            "['input', 'atomic', 'number', '1']\n",
            "['input', 'element', '2']\n",
            "['input', 'multiplicity', '2']\n",
            "['input', 'atomic', 'number', '2']\n",
            "['input', 'element', '3']\n",
            "['input', 'multiplicity', '3']\n",
            "['input', 'atomic', 'number', '3']\n",
            "['input', 'element', '4']\n",
            "['input', 'multiplicity', '4']\n",
            "['input', 'atomic', 'number', '4']\n",
            "['input', 'element', '5']\n",
            "['input', 'multiplicity', '5']\n",
            "['input', 'atomic', 'number', '5']\n",
            "['input', 'element', '6']\n",
            "['input', 'multiplicity', '6']\n",
            "['input', 'atomic', 'number', '6']\n",
            "['input', 'element', '7']\n",
            "['input', 'multiplicity', '7']\n",
            "['input', 'atomic', 'number', '7']\n",
            "['input', 'element', '8']\n",
            "['input', 'multiplicity', '8']\n",
            "['input', 'atomic', 'number', '8']\n",
            "['input', 'element', '9']\n",
            "['input', 'multiplicity', '9']\n",
            "['input', 'atomic', 'number', '9']\n",
            "['input', 'element', '10']\n",
            "['input', 'multiplicity', '10']\n",
            "['input', 'atomic', 'number', '10']\n",
            "['input', 'element', '11']\n",
            "['input', 'multiplicity', '11']\n",
            "['input', 'atomic', 'number', '11']\n",
            "['input', 'element', '12']\n",
            "['input', 'multiplicity', '12']\n",
            "['input', 'atomic', 'number', '12']\n",
            "['input', 'element', '13']\n",
            "['input', 'multiplicity', '13']\n",
            "['input', 'atomic', 'number', '13']\n",
            "['input', 'element', '14']\n",
            "['input', 'multiplicity', '14']\n",
            "['input', 'atomic', 'number', '14']\n",
            "['input', 'element', '15']\n",
            "['input', 'multiplicity', '15']\n",
            "['input', 'atomic', 'number', '15']\n",
            "['input', 'element', '16']\n",
            "['input', 'multiplicity', '16']\n",
            "['input', 'atomic', 'number', '16']\n",
            "['input', 'element', '17']\n",
            "['input', 'multiplicity', '17']\n",
            "['input', 'atomic', 'number', '17']\n",
            "['input', 'element', '18']\n",
            "['input', 'multiplicity', '18']\n",
            "['input', 'atomic', 'number', '18']\n",
            "['input', 'element', '19']\n",
            "['input', 'multiplicity', '19']\n",
            "['input', 'atomic', 'number', '19']\n",
            "['input', 'element', '20']\n",
            "['input', 'multiplicity', '20']\n",
            "['input', 'atomic', 'number', '20']\n",
            "['input', 'element', '21']\n",
            "['input', 'multiplicity', '21']\n",
            "['input', 'atomic', 'number', '21']\n",
            "['input', 'element', '22']\n",
            "['input', 'multiplicity', '22']\n",
            "['input', 'atomic', 'number', '22']\n",
            "['input', 'element', '23']\n",
            "['input', 'multiplicity', '23']\n",
            "['input', 'atomic', 'number', '23']\n",
            "['input', 'element', '24']\n",
            "['input', 'multiplicity', '24']\n",
            "['input', 'atomic', 'number', '24']\n",
            "['input', 'element', '25']\n",
            "['input', 'multiplicity', '25']\n",
            "['input', 'atomic', 'number', '25']\n",
            "['input', 'element', '26']\n",
            "['input', 'multiplicity', '26']\n",
            "['input', 'atomic', 'number', '26']\n",
            "['input', 'element', '27']\n",
            "['input', 'multiplicity', '27']\n",
            "['input', 'atomic', 'number', '27']\n",
            "['input', 'element', '28']\n",
            "['input', 'multiplicity', '28']\n",
            "['input', 'atomic', 'number', '28']\n",
            "['input', 'element', '29']\n",
            "['input', 'multiplicity', '29']\n",
            "['input', 'atomic', 'number', '29']\n",
            "['input', 'element', '30']\n",
            "['input', 'multiplicity', '30']\n",
            "['input', 'atomic', 'number', '30']\n",
            "['input', 'element', '31']\n",
            "['input', 'multiplicity', '31']\n",
            "['input', 'atomic', 'number', '31']\n",
            "['input', 'element', '32']\n",
            "['input', 'multiplicity', '32']\n",
            "['input', 'atomic', 'number', '32']\n",
            "['input', 'element', '33']\n",
            "['input', 'multiplicity', '33']\n",
            "['input', 'atomic', 'number', '33']\n",
            "['input', 'element', '34']\n",
            "['input', 'multiplicity', '34']\n",
            "['input', 'atomic', 'number', '34']\n",
            "['input', 'element', '35']\n",
            "['input', 'multiplicity', '35']\n",
            "['input', 'atomic', 'number', '35']\n",
            "['output', 'element', '1']\n",
            "['output', 'multiplicity', '1']\n",
            "['output', 'atomic', 'number', '1']\n",
            "['output', 'element', '2']\n",
            "['output', 'multiplicity', '2']\n",
            "['output', 'atomic', 'number', '2']\n",
            "['output', 'element', '3']\n",
            "['output', 'multiplicity', '3']\n",
            "['output', 'atomic', 'number', '3']\n",
            "['output', 'element', '4']\n",
            "['output', 'multiplicity', '4']\n",
            "['output', 'atomic', 'number', '4']\n",
            "['output', 'element', '5']\n",
            "['output', 'multiplicity', '5']\n",
            "['output', 'atomic', 'number', '5']\n",
            "['output', 'element', '6']\n",
            "['output', 'multiplicity', '6']\n",
            "['output', 'atomic', 'number', '6']\n",
            "['output', 'element', '7']\n",
            "['output', 'multiplicity', '7']\n",
            "['output', 'atomic', 'number', '7']\n",
            "['output', 'element', '8']\n",
            "['output', 'multiplicity', '8']\n",
            "['output', 'atomic', 'number', '8']\n",
            "['output', 'element', '9']\n",
            "['output', 'multiplicity', '9']\n",
            "['output', 'atomic', 'number', '9']\n",
            "['output', 'element', '10']\n",
            "['output', 'multiplicity', '10']\n",
            "['output', 'atomic', 'number', '10']\n",
            "['output', 'element', '11']\n",
            "['output', 'multiplicity', '11']\n",
            "['output', 'atomic', 'number', '11']\n",
            "['output', 'element', '12']\n",
            "['output', 'multiplicity', '12']\n",
            "['output', 'atomic', 'number', '12']\n",
            "['output', 'element', '13']\n",
            "['output', 'multiplicity', '13']\n",
            "['output', 'atomic', 'number', '13']\n",
            "['output', 'element', '14']\n",
            "['output', 'multiplicity', '14']\n",
            "['output', 'atomic', 'number', '14']\n",
            "['output', 'element', '15']\n",
            "['output', 'multiplicity', '15']\n",
            "['output', 'atomic', 'number', '15']\n",
            "['output', 'element', '16']\n",
            "['output', 'multiplicity', '16']\n",
            "['output', 'atomic', 'number', '16']\n",
            "['output', 'element', '17']\n",
            "['output', 'multiplicity', '17']\n",
            "['output', 'atomic', 'number', '17']\n",
            "['output', 'element', '18']\n",
            "['output', 'multiplicity', '18']\n",
            "['output', 'atomic', 'number', '18']\n",
            "['output', 'element', '19']\n",
            "['output', 'multiplicity', '19']\n",
            "['output', 'atomic', 'number', '19']\n",
            "['output', 'element', '20']\n",
            "['output', 'multiplicity', '20']\n",
            "['output', 'atomic', 'number', '20']\n",
            "['output', 'element', '21']\n",
            "['output', 'multiplicity', '21']\n",
            "['output', 'atomic', 'number', '21']\n",
            "['output', 'element', '22']\n",
            "['output', 'multiplicity', '22']\n",
            "['output', 'atomic', 'number', '22']\n",
            "['output', 'element', '23']\n",
            "['output', 'multiplicity', '23']\n",
            "['output', 'atomic', 'number', '23']\n",
            "['output', 'element', '24']\n",
            "['output', 'multiplicity', '24']\n",
            "['output', 'atomic', 'number', '24']\n",
            "['output', 'element', '25']\n",
            "['output', 'multiplicity', '25']\n",
            "['output', 'atomic', 'number', '25']\n",
            "['output', 'element', '26']\n",
            "['output', 'multiplicity', '26']\n",
            "['output', 'atomic', 'number', '26']\n",
            "['output', 'element', '27']\n",
            "['output', 'multiplicity', '27']\n",
            "['output', 'atomic', 'number', '27']\n",
            "['output', 'element', '28']\n",
            "['output', 'multiplicity', '28']\n",
            "['output', 'atomic', 'number', '28']\n",
            "['output', 'element', '29']\n",
            "['output', 'multiplicity', '29']\n",
            "['output', 'atomic', 'number', '29']\n",
            "['output', 'element', '30']\n",
            "['output', 'multiplicity', '30']\n",
            "['output', 'atomic', 'number', '30']\n",
            "['output', 'element', '31']\n",
            "['output', 'multiplicity', '31']\n",
            "['output', 'atomic', 'number', '31']\n",
            "['output', 'element', '32']\n",
            "['output', 'multiplicity', '32']\n",
            "['output', 'atomic', 'number', '32']\n",
            "['output', 'element', '33']\n",
            "['output', 'multiplicity', '33']\n",
            "['output', 'atomic', 'number', '33']\n",
            "['output', 'element', '34']\n",
            "['output', 'multiplicity', '34']\n",
            "['output', 'atomic', 'number', '34']\n",
            "['output', 'element', '35']\n",
            "['output', 'multiplicity', '35']\n",
            "['output', 'atomic', 'number', '35']\n",
            "['output', 'element', '36']\n",
            "['output', 'multiplicity', '36']\n",
            "['output', 'atomic', 'number', '36']\n",
            "['output', 'element', '37']\n",
            "['output', 'multiplicity', '37']\n",
            "['output', 'atomic', 'number', '37']\n",
            "['output', 'element', '38']\n",
            "['output', 'multiplicity', '38']\n",
            "['output', 'atomic', 'number', '38']\n",
            "['output', 'element', '39']\n",
            "['output', 'multiplicity', '39']\n",
            "['output', 'atomic', 'number', '39']\n",
            "['output', 'element', '40']\n",
            "['output', 'multiplicity', '40']\n",
            "['output', 'atomic', 'number', '40']\n",
            "['output', 'element', '41']\n",
            "['output', 'multiplicity', '41']\n",
            "['output', 'atomic', 'number', '41']\n",
            "['output', 'element', '42']\n",
            "['output', 'multiplicity', '42']\n",
            "['output', 'atomic', 'number', '42']\n",
            "['output', 'element', '43']\n",
            "['output', 'multiplicity', '43']\n",
            "['output', 'atomic', 'number', '43']\n",
            "['output', 'element', '44']\n",
            "['output', 'multiplicity', '44']\n",
            "['output', 'atomic', 'number', '44']\n",
            "['output', 'element', '45']\n",
            "['output', 'multiplicity', '45']\n",
            "['output', 'atomic', 'number', '45']\n",
            "['output', 'element', '46']\n",
            "['output', 'multiplicity', '46']\n",
            "['output', 'atomic', 'number', '46']\n",
            "['output', 'element', '47']\n",
            "['output', 'multiplicity', '47']\n",
            "['output', 'atomic', 'number', '47']\n",
            "['output', 'element', '48']\n",
            "['output', 'multiplicity', '48']\n",
            "['output', 'atomic', 'number', '48']\n",
            "['output', 'element', '49']\n",
            "['output', 'multiplicity', '49']\n",
            "['output', 'atomic', 'number', '49']\n",
            "['output', 'element', '50']\n",
            "['output', 'multiplicity', '50']\n",
            "['output', 'atomic', 'number', '50']\n",
            "['output', 'element', '51']\n",
            "['output', 'multiplicity', '51']\n",
            "['output', 'atomic', 'number', '51']\n",
            "['output', 'element', '52']\n",
            "['output', 'multiplicity', '52']\n",
            "['output', 'atomic', 'number', '52']\n",
            "['output', 'element', '53']\n",
            "['output', 'multiplicity', '53']\n",
            "['output', 'atomic', 'number', '53']\n",
            "['output', 'element', '54']\n",
            "['output', 'multiplicity', '54']\n",
            "['output', 'atomic', 'number', '54']\n",
            "['output', 'element', '55']\n",
            "['output', 'multiplicity', '55']\n",
            "['output', 'atomic', 'number', '55']\n",
            "['output', 'element', '56']\n",
            "['output', 'multiplicity', '56']\n",
            "['output', 'atomic', 'number', '56']\n",
            "['output', 'element', '57']\n",
            "['output', 'multiplicity', '57']\n",
            "['output', 'atomic', 'number', '57']\n",
            "['output', 'element', '58']\n",
            "['output', 'multiplicity', '58']\n",
            "['output', 'atomic', 'number', '58']\n",
            "['output', 'element', '59']\n",
            "['output', 'multiplicity', '59']\n",
            "['output', 'atomic', 'number', '59']\n",
            "['output', 'element', '60']\n",
            "['output', 'multiplicity', '60']\n",
            "['output', 'atomic', 'number', '60']\n",
            "['output', 'element', '61']\n",
            "['output', 'multiplicity', '61']\n",
            "['output', 'atomic', 'number', '61']\n",
            "['output', 'element', '62']\n",
            "['output', 'multiplicity', '62']\n",
            "['output', 'atomic', 'number', '62']\n",
            "['output', 'element', '63']\n",
            "['output', 'multiplicity', '63']\n",
            "['output', 'atomic', 'number', '63']\n",
            "['output', 'element', '64']\n",
            "['output', 'multiplicity', '64']\n",
            "['output', 'atomic', 'number', '64']\n",
            "['output', 'element', '65']\n",
            "['output', 'multiplicity', '65']\n",
            "['output', 'atomic', 'number', '65']\n",
            "['output', 'element', '66']\n",
            "['output', 'multiplicity', '66']\n",
            "['output', 'atomic', 'number', '66']\n",
            "['output', 'element', '67']\n",
            "['output', 'multiplicity', '67']\n",
            "['output', 'atomic', 'number', '67']\n",
            "['output', 'element', '68']\n",
            "['output', 'multiplicity', '68']\n",
            "['output', 'atomic', 'number', '68']\n",
            "['output', 'element', '69']\n",
            "['output', 'multiplicity', '69']\n",
            "['output', 'atomic', 'number', '69']\n",
            "['output', 'element', '70']\n",
            "['output', 'multiplicity', '70']\n",
            "['output', 'atomic', 'number', '70']\n",
            "['output', 'element', '71']\n",
            "['output', 'multiplicity', '71']\n",
            "['output', 'atomic', 'number', '71']\n",
            "['output', 'element', '72']\n",
            "['output', 'multiplicity', '72']\n",
            "['output', 'atomic', 'number', '72']\n",
            "['output', 'element', '73']\n",
            "['output', 'multiplicity', '73']\n",
            "['output', 'atomic', 'number', '73']\n",
            "['output', 'element', '74']\n",
            "['output', 'multiplicity', '74']\n",
            "['output', 'atomic', 'number', '74']\n",
            "0                                        2 Li2CO3 + 5 TiO2\n",
            "1                                   1 NH4H2PO4 + 4.5 Nb2O5\n",
            "2             1 LiOH + 0.95 Ni(OH)2 + 0.225 O2 + 0.05 TiO2\n",
            "3        0.1 Ca(NO3)2 + 1 Fe(NO3)3·9H2O + 0.475 O2 + 0....\n",
            "4                      1 Al(NO3)3·9H2O + 1 LiNO3 + 4 [OH-]\n",
            "                               ...                        \n",
            "19313                                    1 In2O3 + 1 SrCO3\n",
            "19314                                     1 La2O3 + 2 ZrO2\n",
            "19315                      1 La2O3 + 0.16 Y2O3 + 1.68 ZrO2\n",
            "19316            1 BaCO3 + 3.5 Fe2O3 + 2.5 MnO2 + 2.5 TiO2\n",
            "19317                      0.25 Fe2O3 + 0.25 Nb2O5 + 1 PbO\n",
            "Name: input_reaction, Length: 19318, dtype: object\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "object of type 'int' has no len()",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/kesava89/repos/ssm-synthesis-data-processing/main.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://coder.de.soanpapdi.cloud/home/kesava89/repos/ssm-synthesis-data-processing/main.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misnan(i):\n\u001b[1;32m     <a href='vscode-notebook-cell://coder.de.soanpapdi.cloud/home/kesava89/repos/ssm-synthesis-data-processing/main.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     input_list \u001b[39m=\u001b[39m []\n\u001b[0;32m---> <a href='vscode-notebook-cell://coder.de.soanpapdi.cloud/home/kesava89/repos/ssm-synthesis-data-processing/main.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39;49m(input_count)):\n\u001b[1;32m     <a href='vscode-notebook-cell://coder.de.soanpapdi.cloud/home/kesava89/repos/ssm-synthesis-data-processing/main.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m         \u001b[39mif\u001b[39;00m df[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minput_multiplicity_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m] \u001b[39m!=\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://coder.de.soanpapdi.cloud/home/kesava89/repos/ssm-synthesis-data-processing/main.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m             in_atom_num \u001b[39m=\u001b[39m df[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minput_atomic_number_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m] \n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
          ]
        }
      ],
      "source": [
        "input_count = 0\n",
        "output_count = 0\n",
        "columns = df.columns\n",
        "for column in columns:\n",
        "    # print(column)\n",
        "    print(column.split(\"_\"))\n",
        "    if column.split(\"_\")[0] == \"input\": \n",
        "        input_count +=1\n",
        "    if column.split(\"_\")[0] == \"output\": \n",
        "        output_count +=1\n",
        "\n",
        "total_input_list =[]\n",
        "for i, temp in enumerate(df[\"sintering_temp\"]):\n",
        "    \n",
        "    if not np.isnan(i):\n",
        "        input_list = []\n",
        "        for j in range(len(input_count)):\n",
        "            \n",
        "            if df[f\"input_multiplicity_{i}\"] !=0:\n",
        "                in_atom_num = df[f\"input_atomic_number_{i}\"] \n",
        "                in_el = df[f\"input_element_{i}\"] \n",
        "                in_mul = df[f\"input_multiplicity_{i}\"] \n",
        "                input_list.append([in_el, in_mul])\n",
        "        \n",
        "        for j in range(len(output_count)):\n",
        "            if  df[f\"input_multiplicity_{i}\"] !=0:\n",
        "                out_atom_num = df[f\"output_atomic_number_{i}\"] \n",
        "                out_el = df[f\"output_element_{i}\"] \n",
        "                out_mul = df[f\"output_multiplicity_{i}\"] \n",
        "                input_list.append([out_el, out_mul])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for element, count in composition.items():\n",
        "    index = periodic_table[element]\n",
        "    vector[index - 1] = count / total_atoms  \n",
        "embedding = np.sum\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOjWWb9uh3pF"
      },
      "outputs": [],
      "source": [
        "class NN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NN, self).__init__()\n",
        "        self.fc1 = nn.Linear(119, 360)\n",
        "        self.fc2 = nn.Linear(360, 180)\n",
        "        self.fc3 = nn.Linear(180, 100)\n",
        "        self.fc4 = nn.Linear(100, 64)\n",
        "        self.fc5 = nn.Linear(64, 32)\n",
        "        self.fc6 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = torch.relu(self.fc4(x))\n",
        "        x = torch.relu(self.fc5(x))\n",
        "        x = self.fc6(x)\n",
        "        return x\n",
        "model = NN()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYA9nB5lh4Tl"
      },
      "outputs": [],
      "source": [
        "def train(train_loader, device, model, optimizer, criterion):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    average_loss = running_loss / len(train_loader)\n",
        "    return average_loss\n",
        "\n",
        "def validate(validation_loader, device, model, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    validation_outputs = []\n",
        "    validation_truth = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in validation_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            validation_outputs.append(outputs.detach().numpy())\n",
        "            validation_truth.append(labels.detach().numpy())\n",
        "\n",
        "    average_loss = running_loss / len(validation_loader)\n",
        "    validation_outputs = np.concatenate(validation_outputs)\n",
        "    validation_truth = np.concatenate(validation_truth)\n",
        "\n",
        "    return average_loss, validation_outputs, validation_truth\n",
        "\n",
        "\n",
        "def test(test_loader, device, model, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    test_outputs = []\n",
        "    test_truth = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            test_outputs.append(outputs.detach().numpy())\n",
        "            test_truth.append(targets.detach().numpy())\n",
        "\n",
        "    average_loss = running_loss / len(test_loader)\n",
        "    test_outputs = np.concatenate(test_outputs)\n",
        "    test_truth = np.concatenate(test_truth)\n",
        "\n",
        "    return average_loss, test_outputs, test_truth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8mFEPZ1gkfO"
      },
      "outputs": [],
      "source": [
        "sampling_size = 6\n",
        "\n",
        "best_vals =[]\n",
        "best_models = []\n",
        "best_tests = []\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_test_split_index = int(0.9 * len(data_list))\n",
        "\n",
        "train_val_data = data_list[:train_test_split_index]\n",
        "\n",
        "test_data = data_list[train_test_split_index:]\n",
        "test_loader = Dataloader(test_data, batch_size=16, shuffle=False)\n",
        "\n",
        "data_len = len(train_val_data)\n",
        "num_batches = data_len // sampling_size\n",
        "indices = np.arange(data_len)\n",
        "for i in range(num_batches):\n",
        "    best_val_loss = np.inf\n",
        "\n",
        "    start_index = i * sampling_size\n",
        "\n",
        "    val_indices = indices[start_index:start_index + sampling_size]\n",
        "\n",
        "    train_indices = np.setdiff1d(indices, val_test_indices)\n",
        "\n",
        "    train_data = [train_val_data[j] for j in train_indices]\n",
        "    val_data = [train_val_data[j] for j in val_indices]\n",
        "\n",
        "    train_loader = Dataloader(train_data, batch_size=16, shuffle=True)\n",
        "    validation_loader = Dataloader(val_data, batch_size=16, shuffle=False)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
        "                                                        factor=0.8, patience=5,\n",
        "                                                        min_lr=0.0000001)\n",
        "    criterion = nn.L1Loss()\n",
        "    best_validation_loss = float('inf')\n",
        "\n",
        "    for epoch in range(1, EPOCHS+1):\n",
        "\n",
        "        model.train()\n",
        "        loss = train(train_loader, device, model, optimizer, criterion)\n",
        "        scheduler.step(loss)\n",
        "\n",
        "        validation_loss, validation_output, validation_truth_temp = validate(validation_loader, device, model, criterion)\n",
        "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {validation_loss:.4f}\")\n",
        "        if validation_loss < best_validation_loss:\n",
        "            best_validation_loss = validation_loss\n",
        "            best_model_state = copy.deepcopy(model.state_dict())\n",
        "            best_model = model\n",
        "            best_val_loss = validation_loss\n",
        "            print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
        "model.load_state_dict(best_model_state)\n",
        "test_loss, test_outputs, test_truth = test(test_loader, device, model, criterion)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_x4G9KvzDJh"
      },
      "outputs": [],
      "source": [
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dval = xgb.DMatrix(X_val, label=y_val)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'eval_metric': 'rmse',\n",
        "    'max_depth': 6,\n",
        "    'eta': 0.1,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8\n",
        "}\n",
        "\n",
        "evals = [(dtrain, 'train'), (dval, 'eval')]\n",
        "\n",
        "num_boost_round = 100\n",
        "early_stopping_rounds = 10\n",
        "model = xgb.train(params, dtrain, num_boost_round, evals, early_stopping_rounds=early_stopping_rounds, verbose_eval=True)\n",
        "\n",
        "y_pred = model.predict(dtest)\n",
        "test_rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "print(f'Test RMSE: {test_rmse:.4f}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
